{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 3, 122)\n",
      "(1682, 1)\n",
      "(412, 3, 122)\n",
      "(412, 1)\n",
      "0\n",
      "Defined 8 models\n",
      ">knn: 59.223\n",
      ">cart: 52.184\n",
      ">svm: 64.563\n",
      ">bayes: 54.369\n",
      ">bag: 59.951\n",
      ">rf: 58.738\n",
      ">et: 59.709\n",
      ">gbm: 58.252\n",
      "\n",
      "Name=svm, Score=64.563\n",
      "Name=bag, Score=59.951\n",
      "Name=et, Score=59.709\n",
      "Name=knn, Score=59.223\n",
      "Name=rf, Score=58.738\n",
      "Name=gbm, Score=58.252\n",
      "Name=bayes, Score=54.369\n",
      "Name=cart, Score=52.184\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from pathlib import Path\n",
    "from numpy import hstack\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_sequences(sequences, n_steps, n_out):\n",
    "\ttrainx, trainy = list(), list()\n",
    "\ttestx, testy = list(), list()\n",
    "\ttestsplit = int((len(sequences)) *.20) # change percentage for test here\n",
    "\ttrain, test = sequences[0:-(testsplit)], sequences[-(testsplit):]\n",
    "   \n",
    "\n",
    "\tfor i in range(len(train)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(train)-(1+n_out):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = train[i:end_ix, :-1], train[end_ix+n_out, -1:]          \n",
    "\t\ttk1 = train[i:end_ix+1+n_out, :-1]\n",
    "\t\tfirst = tk1[0,0]\n",
    "\t\tlast =  tk1[n_steps+n_out,0]        \n",
    "\t\tif first==last:\n",
    "\t\t\ttrainy.append(seq_y)\n",
    "\t\t\ttrainx.append(seq_x[:,2:])\n",
    "\t\telse:\n",
    "\t\t\tNone \n",
    "            \n",
    "\tfor i in range(len(test)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(test)-(1+n_out):  #leave # of x rows on bottom for process to stop\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = test[i:end_ix, :-1], test[end_ix+n_out, -1:]    #get batch of n_steps for X, batch of y data is plus 1 (or x) vertically      \n",
    "\t\ttk1 = test[i:end_ix+1+n_out, :-1]     # get the batch above plus 1 (or x) more rows to test if belong to same ticker\n",
    "\t\tfirst = tk1[0,0]\n",
    "\t\tlast =  tk1[n_steps+n_out,0]        \n",
    "\t\tif first==last:\n",
    "\t\t\ttesty.append(seq_y)\n",
    "\t\t\ttestx.append(seq_x[:,2:])\n",
    "\t\telse:\n",
    "\t\t\tNone\n",
    "\treturn array(trainx), array(trainy), array(testx), array(testy)\n",
    "\n",
    "\n",
    "\n",
    "# the path to csv file directory\n",
    "mycsvdir = 'C:/Users/singh/Documents/rawdata/test'\n",
    "\n",
    "# get all the csv files in that directory (assuming they have the extension .csv)\n",
    "csvfiles = glob.glob(os.path.join(mycsvdir, '*.csv'))\n",
    "\n",
    "# loop through the files and read them in with pandas\n",
    "dataframes = []  # a list to hold all the individual pandas DataFrames\n",
    "for csvfile in csvfiles:\n",
    "    \n",
    "    #read data, clean hyphen and text, remove date columns, convert string object to float, remove redundant columns, average price, remove high/low price columns, append to dataframe\n",
    "    rawdata = pd.read_csv(csvfile)\n",
    "    dataset = rawdata.iloc[18:230, 33:153]\n",
    "    dft = dataset.T\n",
    "    #breakpoint()\n",
    "    base, ext = os.path.splitext(Path(csvfile).name)\n",
    "    dft[18] = base\n",
    "    dft = dft.replace('-', 0)\n",
    "    dft = dft.replace('Negative Tangible Equity', 0)\n",
    "    dft = dft.replace('No Debt', 0)\n",
    "    dataframes.append(dft)\n",
    "    \n",
    "# concatenate them all together\n",
    "result = pd.concat(dataframes, ignore_index=True)\n",
    "result = result.drop(columns=[21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,73,79,80,82,84,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,217,218,219,220,221,222,225,226,227,228,229])\n",
    "result[[42,59,60,61,62,63,64,65,66,67,68,69,70,71,72,74,75,76,77,78,81,83,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,197,215,216,223,224]] = result[[42,59,60,61,62,63,64,65,66,67,68,69,70,71,72,74,75,76,77,78,81,83,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,197,215,216,223,224]].astype('float32')\n",
    "result[230] = result[[215, 216]].mean(axis=1)\n",
    "result = result.drop(columns=[215, 216])\n",
    "result = result[result[230] > 0]\n",
    "result = result.dropna()\n",
    "result[231] = result[230].pct_change().astype('float32')\n",
    "result[231].fillna(0, inplace=True)\n",
    "\n",
    "result[231] = np.where(result[231]>.05, 1, result[231])\n",
    "#result[231] = np.where(result[231].between(.25,1), 6, result[231])\n",
    "#result[231] = np.where(result[231].between(-.05,.05), 0, result[231])\n",
    "\n",
    "result[231] = np.where(result[231]<.05, -1, result[231])\n",
    "#result[231] = np.where(result[231].between(-1,-.25), -6, result[231])\n",
    "#result[231] = np.where(result[231].between(-.05,-0.00000000001), -1, result[231])\n",
    "\n",
    "result.to_csv('all.csv')\n",
    "result = result.drop(columns=[230])\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "\t# nonlinear models\n",
    "\tmodels['knn'] = KNeighborsClassifier(n_neighbors=14)\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['svm'] = SVC()\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\t# ensemble models\n",
    "\tmodels['bag'] = BaggingClassifier(n_estimators=400)\n",
    "\tmodels['rf'] = RandomForestClassifier(n_estimators=400)\n",
    "\tmodels['et'] = ExtraTreesClassifier(n_estimators=400)\n",
    "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=400)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "\t# fit the model\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make predictions\n",
    "\tyhat = model.predict(testX)\n",
    "\t# evaluate predictions\n",
    "\taccuracy = accuracy_score(testy, yhat)\n",
    "\treturn accuracy * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate the model\n",
    "\t\tresults[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "\t\t# show process\n",
    "\t\tprint('>%s: %.3f' % (name, results[name]))\n",
    "\treturn results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,v) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\tprint()\n",
    "\tfor name, score in mean_scores:\n",
    "\t\tprint('Name=%s, Score=%.3f' % (name, score))\n",
    "\n",
    "n_steps = 3\n",
    "n_out = 0    # 0 is t+1, 1 is next one t+2\n",
    "# convert into input/output\n",
    "trainX, trainy, testX, testy  = split_sequences(result.values, n_steps, n_out)\n",
    "\n",
    "print(trainX.shape)\n",
    "print(trainy.shape)\n",
    "print(testX.shape)\n",
    "print(testy.shape)\n",
    "print(n_out)\n",
    "\n",
    "\t# flatten X\n",
    "trainX = trainX.reshape((trainX.shape[0], trainX.shape[1] * trainX.shape[2]))\n",
    "trainX = trainX.astype('float32')\n",
    "trainX = trainX.astype('int')\n",
    "testX = testX.reshape((testX.shape[0], testX.shape[1] * testX.shape[2]))\n",
    "testX = testX.astype('float32')\n",
    "testX = testX.astype('int')\n",
    "\t# flatten y\n",
    "trainy, testy = trainy[:,0], testy[:,0]\n",
    "trainy = trainy.astype('int')\n",
    "testy = testy.astype('int')\n",
    "\n",
    "r_scaler = preprocessing.Normalizer()\n",
    "trainX = r_scaler.fit_transform(trainX)\n",
    "testX = r_scaler.fit_transform(testX)\n",
    "\n",
    "#r_scaler = preprocessing.StandardScaler()\n",
    "#trainX = r_scaler.fit_transform(trainX)\n",
    "#testX = r_scaler.fit_transform(testX)\n",
    "\n",
    "# get model list\n",
    "models = define_models()\n",
    "# evaluate models\n",
    "results = evaluate_models(trainX, trainy, testX, testy, models)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
